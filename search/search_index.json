{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Stock RaceTrack","text":"<p>My stock research toolset.</p> <p>The project is under active development. See Blogs if you're interested in the development progress.</p>"},{"location":"blog/","title":"Blog","text":"<p>Here docs the techincal thoughts and solution of the difficulties met when building this toolset.</p>"},{"location":"blog/2025/09/23/difficulties-of-implementing-a-data-downloader/","title":"Difficulties of Implementing a Data Downloader","text":"<p>HTTP based APIs (like Tushare and AKshare) are very convenient, but when it comes to stock research, backtesting or auto-trading, they have some fatal drawbacks:</p> <ol> <li>Network delay: the network delay would accumulate to an unacceptable level when the computation involves many queries.</li> <li>API limitation: most of the public data API endpoint has a low limitation of fetching data.</li> </ol> <p>To overcome these difficulties, one solution is to \"cached\" them in your private database.</p>"},{"location":"blog/2025/09/23/difficulties-of-implementing-a-data-downloader/#nature-of-web-based-api","title":"Nature of Web-based API","text":"<p>The nature of all APIs are:</p> <ol> <li>Long RTT(round-trip-time)</li> <li>Low request frequency</li> <li> <p>Limited records returned</p> </li> <li> <p>To overcome 1, the solution is parallelization and cache.</p> </li> <li>To overcome 2, the solution would be batch and cache.</li> <li>To overcome 3, we use batch with limited size.</li> </ol>"},{"location":"blog/2025/09/23/difficulties-of-implementing-a-data-downloader/#nature-of-stock-data-api","title":"Nature of Stock Data API","text":"<p>Most Stock data APIs can be categorized as:</p> API params \\ Data indexed by symbol &amp; timestamp symbol or timestamp depending on the size of the data symbols(s) by symbol timestamp / timerange by timerange <p>So Stock Data API can be defined as:</p> <pre><code>class API:\n{\n  \"biz_key\": \"tushare_daily\"\n  \"fetch_methods\": {\n    \"by_time\": by_time # None | Callable[str, start, end]\n    \"by_symbol\": by_symbol # None | Callable[list[str], start, end]\n  },\n  \"limit\": {\n    \"qps\": 10 # query per second, int\n    \"rpq\": 6000 # response per query, int\n  },\n  \"frequency\": timedelta(1) # frequencies of time.\n}\n</code></pre>"},{"location":"blog/2025/09/23/difficulties-of-implementing-a-data-downloader/#variety-of-data-schema","title":"Variety of data schema","text":"<p>For stock data, there're so many data endpoints with so many different schemas. It's not realistic to replicate such amount of APIs. But we can directly store each data record as a json object to decouple the fetching and preprocessing stage.</p> <p>Most of the stock data types are indexed by two dimension: time and symbol. The symbol can be stock, fund, index or something else.</p> <p>So each record in the table would be:</p> <p>(biz_key, symbol, timestamp, data) or (biz_key, symbol, timerange, data)</p> <p>timestamp can be represented by timerange as long as we state that the left endpoint of the timerange is the timestamp to be represented, so record in the table could be:</p> <pre><code>record = (biz_key, symbol, timerange, data)\n</code></pre> <p>, where <code>biz_key</code> is the identifier of the API.</p> <p>With this schema, we can adapt to many different API of many different data providers easily.</p>"},{"location":"blog/2025/09/23/difficulties-of-implementing-a-data-downloader/#api-abuse-meaning-less-request","title":"API abuse: Meaning less request","text":"<p>The data request is usually like this:</p> <pre><code>request = (biz_key, symbol_set, timerange)\n</code></pre> <p>But when most of the data in the request is already fetched, fetching all data specified by the request would be wasteful.</p> <p>What's more, the underlying API might be frequency limited or returning record limited, which further complicates the problem.</p>"},{"location":"blog/2025/09/23/difficulties-of-implementing-a-data-downloader/#solution-1-brute-force","title":"Solution 1: brute force","text":"<p>We assume that the data provider can provide both fetch-by-time and fetch-by-symbol methods, the size of the whole dataset is acceptable for both storage and time to fetch. Then we just need to:</p> <ol> <li>assume that all presented symbol is up-to-date for the last update time</li> <li>and just fetch full history of those missing symbols till the last update time</li> <li>and finally fetch all data from last update time to now by time.</li> </ol>"},{"location":"blog/2025/09/23/difficulties-of-implementing-a-data-downloader/#solution-2-fine-grianed-control","title":"Solution 2: fine grianed control","text":"<p>Remember our purpose: we want to avoid abusing the API. So the target is not to avoid all meaningless request, but try to cut as more meaningless request as possible.</p> <p>To avoid abusing the API, the solution is merging the underlying data request. But before we can merge the data requests, we have to find them out first.</p>"},{"location":"blog/2025/09/23/difficulties-of-implementing-a-data-downloader/#tracking-the-missing-data","title":"Tracking the missing data","text":"<p>One solution is, for each <code>(biz_key, symbol)</code>, maintaining a table <code>raw_data_coverage</code> recording the data fetched during <code>timerange</code>. Every time we bulk write the <code>raw_data</code> table, we also bulk request and update <code>raw_data_ccoverage</code>. And every time we update the table, we spend <code>O(S)</code> time to calculate the indexes of missing records, where <code>S</code> is the size of the symbol set given.</p> <p>There're some corner stage. Some data is missing because of its nature: the market is close on that day, the company is unlisted, not listed, suspended and so on. So the calculated set is a superset of the missing data. The analyze and solution can be found below</p>"},{"location":"blog/2025/09/23/difficulties-of-implementing-a-data-downloader/#the-merging-direction","title":"The merging direction","text":"<p>Different API suits different merging direction. If the API is a crawler that craw detail page of a specific symbol, then we should merge in the time direction; if the API can provide a fetch several symbols in batch, then merging in the symbol direction is also a good choice. As long as the stride is under the limit specified by the API, merge all requests.</p> <p>But which are better? My solution is: take both and compare. The complexity is just <code>O(T) + O(S)</code>. So the computing costs is much lower then the <code>O(TS)</code> fetching costs.</p> <p>One corner case is that the missing data is distributed evenly. This would trigger a fetch of all existing data. But since the request is both symbol locally and time locally, this case would be rare.</p>"},{"location":"blog/2025/09/23/difficulties-of-implementing-a-data-downloader/#known-missing-data","title":"Known missing data","text":"<p>Some data does not exists in the certain time range. If we ignore this nature of the stock data, we will end up with a fragmented <code>raw_data_coverage</code> table. Until now, the semantic of the coverage table is: the time range the data covered. But to avoid fragmentation, and to make the table better serve the purpose of avoid duplicate API calls, let us shift the semantic to \"the time range that do not fetch again\".</p> <p>But how/when to declare these time ranges? There're two approaches, a strict approach and a lapse approach. If we take the strict approach, we need a trustful source that tell us that which ranges are not covered. If we take the lapse approach, we just simply black out all those we have tried, and those return an empty data.</p> <p>The later one is more pratical: if no data is returned, just assume the data does not exists. Data in the past logically will not be updated in the future. So missing data is always missing.</p>"},{"location":"blog/2025/09/28/pyramid-strategy/","title":"Pyramid Strategy","text":"<p>The core idea of the pyramid strategy is to divide your capital into several lots, with each subsequent lot being larger than the previous one. You start by buying a small lot at a higher price, then buy larger lots as the price decreases. Conversely, you sell small lots at lower prices and larger lots as the price rises. This approach aims to accumulate positions at lower average prices and realize profits by selling at higher average prices.</p> <ol> <li>The buying price should be a low price.</li> <li>There must be an opportunity that can help people sell at a higher price.</li> </ol>"},{"location":"blog/2025/09/28/pyramid-strategy/#naive-pyramid","title":"Naive Pyramid","text":"<p>The algorithm should go like this:</p> <ol> <li>Buy if no position held.</li> <li>if so, start buying the stock, everytime we buy, we use the smallest slot available.</li> <li>if the stock price rise and reach our take profit limit, start selling with the smallest occupied lot, and only sell more stocks as the stock rise even higher.</li> <li>Buy more stocks only when the stock price dive into a price lower than previous buy.</li> </ol> <p>But how to decide whether the stock is worth buying? It's not about pyramid; it's another aspect of strategy. The function of pyramid strategy is just to help you avoid the risk.</p>"},{"location":"blog/2025/09/28/pyramid-strategy/#an-application-of-pyramid","title":"An application of Pyramid","text":"<p>To find a good starting buy point, we can have different methodologies. One of the useful strategies I'm quite statisfied with is:</p> <ol> <li>buy if the low price is lower than the bollinger's lower line</li> <li>sell if the high price is higher than the bollinger's middle line.</li> </ol>"},{"location":"blog/2025/09/28/pyramid-strategy/#limitation","title":"Limitation","text":"<p>Pyramid strategy is good method that help manage position, but it can't prevent you buy at a maxima of price. To find a good buying point and avoid sold too early, we should design a more delicate mechanism.</p>"},{"location":"blog/archive/2025/","title":"September 2025","text":""},{"location":"blog/category/research/","title":"Research","text":""},{"location":"blog/category/data/","title":"Data","text":""}]}