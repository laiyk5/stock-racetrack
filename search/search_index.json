{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Stock RaceTrack","text":"<p>My stock research toolset.</p> <p>The project is under active development. See Blogs if you're interested in the development progress.</p>"},{"location":"api/","title":"API","text":""},{"location":"blog/","title":"Blog","text":""},{"location":"blog/2025/09/30/implementing-a-pyramid-strategy/","title":"Implementing a Pyramid Strategy","text":"<p>The core idea of the pyramid strategy is to divide your capital into several lots, with each subsequent lot being larger than the previous one. You start by buying a small lot at a higher price, then buy larger lots as the price decreases. Conversely, you sell small lots at lower prices and larger lots as the price rises. This approach aims to accumulate positions at lower average prices and realize profits by selling at higher average prices.</p> <ol> <li>The buying price should be a low price.</li> <li>There must be an opportunity that can help people sell at a higher price.</li> </ol>"},{"location":"blog/2025/09/30/implementing-a-pyramid-strategy/#naive-pyramid","title":"Naive Pyramid","text":"<p>The algorithm should go like this:</p> <ol> <li>Buy if no position held.</li> <li>if so, start buying the stock, everytime we buy, we use the smallest slot available.</li> <li>if the stock price rise and reach our take profit limit, start selling with the smallest occupied lot, and only sell more stocks as the stock rise even higher.</li> <li>Buy more stocks only when the stock price dive into a price lower than previous buy.</li> </ol> <p>But how to decide whether the stock is worth buying? It's not about pyramid; it's another aspect of strategy. The function of pyramid strategy is just to help you avoid the risk.</p>"},{"location":"blog/2025/09/30/implementing-a-pyramid-strategy/#an-application-of-pyramid","title":"An application of Pyramid","text":"<p>To find a good starting buy point, we can have different methodologies. One of the useful strategies I'm quite statisfied with is:</p> <ol> <li>buy if the low price is lower than the bollinger's lower line</li> <li>sell if the high price is higher than the bollinger's middle line.</li> </ol>"},{"location":"blog/2025/09/30/implementing-a-pyramid-strategy/#limitation","title":"Limitation","text":"<p>Pyramid strategy is good method that help manage position, but it can't prevent you buy at a maxima of price. To find a good buying point and avoid sold too early, we should design a more delicate mechanism.</p>"},{"location":"blog/2025/09/23/trading-research/","title":"Trading Research","text":"<p>Every trader want a robot (autotrader) that earns money for them automatically, so they don't have to stair at the screen all the time.</p> <p>But how to start a trading research?</p>"},{"location":"blog/2025/09/23/trading-research/#a-simple-idea","title":"A simple idea","text":"<p>The life cycle of an autotrader would be:</p> <ol> <li>We develop an autotrader</li> <li>We backtest it on historical data</li> <li>If good, test it in real life or directly put your money in</li> <li>It underperform, so you come back to fix it.</li> </ol> <p>The tools required are:</p> <ol> <li>A datasource: a method to easily reach a variety of data with low latency and convinience.</li> <li>A backtesting tool</li> <li>An API to your account (If you want to risk your money)</li> </ol> <p>Datasource is the most tricky part for independent researcher like me. They typically have very limited funding, so they cannot afford high quality, high availability datasource. But luckily, many web-based data providers are available.</p> <p>As to backtesting tool, unluckily, as far as I know, there're no free full-fledge backtesting tool available. Most of the backtesting tool are still at toy level. They can only run a strategy on a small set of symbols.</p>"},{"location":"blog/2025/09/23/trading-research/#a-simple-architecture-of-autotrader","title":"A simple architecture of autotrader","text":"<p>The simplest form of autotrader is that we allocate one autotrader for every stock in the market, and each autotrader focuses on only one stock.</p> <p>But how do we allocate our money? And if</p>"},{"location":"blog/2025/09/23/trading-research/#a-series-of-questions","title":"A series of questions","text":"<p>When we trade in the market, we always want to keep our portfolio most profitable in the long run.</p> <ol> <li>How do we buy and sell?<ol> <li>should we set a stop profit and a stop loss limit?</li> <li>what's the reason supporting our decision?<ol> <li>technical factors: stock price? marginal capitals? moneyflow?</li> <li>basic factors: the value of the company? the profitability?</li> <li>market motion / industry tendency: If the market mood is good? If the industry is thriving? If the company is attched with some label that very popular among investers?</li> <li>big event: if the company is related to an uncoming event or emergencies that has not been realized?</li> </ol> </li> </ol> </li> <li>How do we allocate our money?<ol> <li>should we limit the highest among of money that the autotrader can use for each stock?</li> <li>should we isolate the available money for each stock?</li> <li>what to do with the profit or loss?</li> </ol> </li> <li>The frequency we trade?<ol> <li>what's the minimum time interval between two trades?<ol> <li>If we're just talking about a single stock, what's the frequecy? 1 minute?</li> <li>If we're talking about the whole market, what's the frequency? 1 hour? 1 day?</li> </ol> </li> <li>how patient we are?<ol> <li>Totally impatient, If I don't earn money in the next second, I just fire the autotrader.</li> <li>Lossing money for a month is acceptable, but not for a session.</li> <li>I'm really patient -- I can afford lossing the money for 3-5 years.</li> </ol> </li> </ol> </li> <li>How to define success?</li> </ol> <p>We don't want to waste our time on a set of slow growing stocks. One problem is the market changes very fast, the decision we made is only valid if the market doesn't change much before we execute the strategy. Another problem is</p>"},{"location":"blog/2025/09/23/difficulties-of-implementing-a-data-downloader/","title":"Difficulties of Implementing a Data Downloader","text":"<p>HTTP based APIs (like Tushare and AKshare) are very convenient, but when it comes to stock research, backtesting or auto-trading, they have some fatal drawbacks:</p> <ol> <li>Network delay: the network delay would accumulate to an unacceptable level when the computation involves many queries.</li> <li>API limitation: most of the public data API endpoint has a low limitation of fetching data.</li> </ol> <p>To overcome these difficulties, one solution is to \"cached\" them in your private database.</p>"},{"location":"blog/2025/09/23/difficulties-of-implementing-a-data-downloader/#nature-of-web-based-api","title":"Nature of Web-based API","text":"<p>The nature of all APIs are:</p> <ol> <li>Long RTT(round-trip-time)</li> <li>Low request frequency</li> <li> <p>Limited records returned</p> </li> <li> <p>To overcome 1, the solution is parallelization and cache.</p> </li> <li>To overcome 2, the solution would be batch and cache.</li> <li>To overcome 3, we use batch with limited size.</li> </ol>"},{"location":"blog/2025/09/23/difficulties-of-implementing-a-data-downloader/#nature-of-stock-data-api","title":"Nature of Stock Data API","text":"<p>Most Stock data APIs can be categorized as:</p> API params \\ Data indexed by symbol &amp; timestamp symbol or timestamp depending on the size of the data symbols(s) by symbol timestamp / timerange by timerange <p>So Stock Data API can be defined as:</p> <pre><code>class API:\n{\n  \"biz_key\": \"tushare_daily\"\n  \"fetch_methods\": {\n    \"by_time\": by_time # None | Callable[str, start, end]\n    \"by_symbol\": by_symbol # None | Callable[list[str], start, end]\n  },\n  \"limit\": {\n    \"qps\": 10 # query per second, int\n    \"rpq\": 6000 # response per query, int\n  },\n  \"frequency\": timedelta(1) # frequencies of time.\n}\n</code></pre>"},{"location":"blog/2025/09/23/difficulties-of-implementing-a-data-downloader/#variety-of-data-schema","title":"Variety of data schema","text":"<p>For stock data, there're so many data endpoints with so many different schemas. It's not realistic to replicate such amount of APIs. But we can directly store each data record as a json object to decouple the fetching and preprocessing stage.</p> <p>Most of the stock data types are indexed by two dimension: time and symbol. The symbol can be stock, fund, index or something else.</p> <p>So each record in the table would be:</p> <p>(biz_key, symbol, timestamp, data) or (biz_key, symbol, timerange, data)</p> <p>timestamp can be represented by timerange as long as we state that the left endpoint of the timerange is the timestamp to be represented, so record in the table could be:</p> <pre><code>record = (biz_key, symbol, timerange, data)\n</code></pre> <p>, where <code>biz_key</code> is the identifier of the API.</p> <p>With this schema, we can adapt to many different API of many different data providers easily.</p>"},{"location":"blog/2025/09/23/difficulties-of-implementing-a-data-downloader/#api-abuse-meaning-less-request","title":"API abuse: Meaning less request","text":"<p>The data request is usually like this:</p> <pre><code>request = (biz_key, symbol_set, timerange)\n</code></pre> <p>But when most of the data in the request is already fetched, fetching all data specified by the request would be wasteful.</p> <p>What's more, the underlying API might be frequency limited or returning record limited, which further complicates the problem.</p>"},{"location":"blog/2025/09/23/difficulties-of-implementing-a-data-downloader/#solution-1-brute-force","title":"Solution 1: brute force","text":"<p>We assume that the data provider can provide both fetch-by-time and fetch-by-symbol methods, the size of the whole dataset is acceptable for both storage and time to fetch. Then we just need to:</p> <ol> <li>assume that all presented symbol is up-to-date for the last update time</li> <li>and just fetch full history of those missing symbols till the last update time</li> <li>and finally fetch all data from last update time to now by time.</li> </ol>"},{"location":"blog/2025/09/23/difficulties-of-implementing-a-data-downloader/#solution-2-fine-grianed-control","title":"Solution 2: fine grianed control","text":"<p>Remember our purpose: we want to avoid abusing the API. So the target is not to avoid all meaningless request, but try to cut as more meaningless request as possible.</p> <p>To avoid abusing the API, the solution is merging the underlying data request. But before we can merge the data requests, we have to find them out first.</p>"},{"location":"blog/2025/09/23/difficulties-of-implementing-a-data-downloader/#tracking-the-missing-data","title":"Tracking the missing data","text":"<p>One solution is, for each <code>(biz_key, symbol)</code>, maintaining a table <code>raw_data_coverage</code> recording the data fetched during <code>timerange</code>. Every time we bulk write the <code>raw_data</code> table, we also bulk request and update <code>raw_data_ccoverage</code>. And every time we update the table, we spend <code>O(S)</code> time to calculate the indexes of missing records, where <code>S</code> is the size of the symbol set given.</p> <p>There're some corner stage. Some data is missing because of its nature: the market is close on that day, the company is unlisted, not listed, suspended and so on. So the calculated set is a superset of the missing data. The analyze and solution can be found below</p>"},{"location":"blog/2025/09/23/difficulties-of-implementing-a-data-downloader/#the-merging-direction","title":"The merging direction","text":"<p>Different API suits different merging direction. If the API is a crawler that craw detail page of a specific symbol, then we should merge in the time direction; if the API can provide a fetch several symbols in batch, then merging in the symbol direction is also a good choice. As long as the stride is under the limit specified by the API, merge all requests.</p> <p>But which are better? My solution is: take both and compare. The complexity is just <code>O(T) + O(S)</code>. So the computing costs is much lower then the <code>O(TS)</code> fetching costs.</p> <p>One corner case is that the missing data is distributed evenly. This would trigger a fetch of all existing data. But since the request is both symbol locally and time locally, this case would be rare.</p>"},{"location":"blog/2025/09/23/difficulties-of-implementing-a-data-downloader/#known-missing-data","title":"Known missing data","text":"<p>Some data does not exists in the certain time range. If we ignore this nature of the stock data, we will end up with a fragmented <code>raw_data_coverage</code> table. Until now, the semantic of the coverage table is: the time range the data covered. But to avoid fragmentation, and to make the table better serve the purpose of avoid duplicate API calls, let us shift the semantic to \"the time range that do not fetch again\".</p> <p>But how/when to declare these time ranges? There're two approaches, a strict approach and a lapse approach. If we take the strict approach, we need a trustful source that tell us that which ranges are not covered. If we take the lapse approach, we just simply black out all those we have tried, and those return an empty data.</p> <p>The later one is more pratical: if no data is returned, just assume the data does not exists. Data in the past logically will not be updated in the future. So missing data is always missing.</p>"},{"location":"blog/2025/12/08/product-design/","title":"Product Design","text":""},{"location":"blog/2025/12/08/product-design/#requirement-analysis","title":"Requirement Analysis","text":"<p>The strategies developing cycle is: collect data, write a strategies, save the running result, evaluate the performance, and compare it with other strategies or on different time ranges. And finally moving the strategy for actual use require more engineering. Maintaining the data is also a pain. So for strategies engineer, they need:</p> <ol> <li>A consistent way of strategy backtesting/simulating/applying</li> <li>A data maintainance tool that will help run the common maintainance routines and warn them if the data is lacked from the database.</li> <li>A clear, easy to use interface that:</li> <li>notify them to interact with the strategy if it's design with a man in the loop.</li> <li>provide a monitor foor the running strategy so developer can understand what's going on wherever they are.</li> <li>provide a data exploration tool so the running results can be saved and the developer can check the results of different runs easily.</li> </ol>"},{"location":"blog/2025/12/08/product-design/#architecture","title":"Architecture","text":"<ul> <li>Data management service: a framework that expose an API for daily data management and availability checking.</li> <li>Strategy service: a framework that execute the plugged strategy on the plugged broker and publish notification using the plugged notifiers. It also expose an API for monitoring and execution management.</li> <li>Web App: provide an Web UI for the two services above. Should be able to remember the user's setting and their configuration. The user should be able to check the performance or the running logs easily.</li> <li>backend: remember the user and its data.</li> <li>UI/frontend</li> </ul>"},{"location":"blog/2025/12/08/product-design/#roadmap","title":"Roadmap","text":"<p>strategy service depends on the data service for the data avialbility checking.</p>"},{"location":"blog/2025/12/08/product-design/#data-management-service","title":"Data management Service","text":"<p>features:</p> <ul> <li> data coverage checking</li> <li> data maintainance routines scheduling configuration.</li> <li> data maintainance routines status checking</li> </ul>"},{"location":"blog/2025/12/08/product-design/#strategy-service","title":"Strategy service","text":"<p>features:</p> <ul> <li> plugable broker (account and way to apply suggestions)</li> <li> plugable strategy (read broker states and offer suggestions)</li> <li> price feed definition</li> <li> decision history checking</li> <li> account history checking</li> <li> notification</li> <li> human-decision making broker</li> </ul>"},{"location":"blog/2025/12/08/product-design/#data-exploration","title":"Data Exploration","text":"<p>Note</p> <p>Should be decide in the future</p>"},{"location":"blog/archive/2025/","title":"2025","text":""},{"location":"blog/category/design/","title":"Design","text":""},{"location":"blog/category/implementation/","title":"Implementation","text":""},{"location":"blog/category/philosophy/","title":"Philosophy","text":""}]}